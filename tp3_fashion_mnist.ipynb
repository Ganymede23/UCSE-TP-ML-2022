{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests GPU local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Determinista\n",
    "# tf.keras.utils.set_random_seed(1)\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPUs:\", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de python, para especificar rutas de archivos y directorios\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# lib para trabajar con arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# lib que usamos para mostrar las imágenes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# libs que usamos para construir y entrenar redes neuronales, y que además tiene utilidades para leer sets de \n",
    "# imágenes\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten, Rescaling\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "# libs que usamos para tareas generales de machine learning. En este caso, métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# configuración para que las imágenes se vean dentro del notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "'''\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "(X_train, Y_train) = train \n",
    "(X_test, Y_test) = test\n",
    "'''\n",
    "train, test = fashion_mnist.load_data()\n",
    "(X_train, Y_train) = train \n",
    "(X_test, Y_test) = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "\n",
    "LABELS = np.unique(Y_train).tolist()\n",
    "INPUTS = 28*28\n",
    "OUTPUTS = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "El dataset se divide en un set de train con 60.000 ejemplos, y un set de test con otros 10.000. Cada ejemplo consta de una imagen en escala de grises de 28x28 pixeles (784 en total), asociado a una etiqueta de 10 clases.\n",
    "Cada pixel es representado por un solo valor, indicando el nivel de brillo u obscuridad en él. Estos valores van entre 0 y 255.\n",
    "\n",
    "### Etiquetas\n",
    "- 0 - T-shirt/top\n",
    "- 1 - Trouser\n",
    "- 2 - Pullover\n",
    "- 3 - Dress\n",
    "- 4 - Coat\n",
    "- 5 - Sandal\n",
    "- 6 - Shirt\n",
    "- 7 - Sneaker\n",
    "- 8 - Bag\n",
    "- 9 - Ankle boot\n",
    "\n",
    "Las dimensiones de las imágenes pueden apreciarse al hacer un `.shape` de los datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:', X_train.shape)\n",
    "print('Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataset): \n",
    "    # specify the number of rows and columns you want to see\n",
    "    num_row = 3\n",
    "    num_col = 3\n",
    "\n",
    "    # get a segment of the dataset\n",
    "    num = num_row*num_col\n",
    "    if dataset == train:\n",
    "        images, labels = X_train[:num], Y_train[:num]\n",
    "    else: # Test dataset\n",
    "        images, labels = X_test[:num], Y_test[:num]\n",
    "\n",
    "    # plot images\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ytrain = pd.DataFrame(data=Y_train)\n",
    "df_ytest = pd.DataFrame(data=Y_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "fig.suptitle('Distribución de la variable target')\n",
    "ax1.bar([0,1,2,3,4,5,6,7,8,9],df_ytrain.value_counts())\n",
    "ax2.bar([0,1,2,3,4,5,6,7,8,9],df_ytest.value_counts())\n",
    "ax1.set_xticks([0,1,2,3,4,5,6,7,8,9])\n",
    "ax2.set_xticks([0,1,2,3,4,5,6,7,8,9])\n",
    "ax1.title.set_text('Train')\n",
    "ax2.title.set_text('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable a predecir tiene una **distribución uniforme** en todo ambos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores a un rango 0-1\n",
    "\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions\n",
    "    https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n",
    "\n",
    "Loss functions\n",
    "    https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "Neurons and layers\n",
    "    https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "https://www.hindawi.com/journals/mpe/2013/425740/\n",
    "https://peerj.com/articles/cs-724/\n",
    "\n",
    "\n",
    "> In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU …\n",
    "\n",
    "— Page 174, Deep Learning, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1_test = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(20, activation='tanh'),\n",
    "    Dense(20, activation='tanh'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_1_test.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_1_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mlp_1_test.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=3,\n",
    "    batch_size=250,\n",
    "    validation_data=(X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests académicos\n",
    "\n",
    "## Masters (1993)\n",
    "\n",
    "Masters (1993) argued that there are no theoretical reasons in using more than two hidden layers in a neural network. Likewise, no practical reasons either for it. Whereas for determining the number of hidden neurons using the geometric pyramid rule, it is stated that the number of neurons for each hidden layer will form the shape of a pyramid, where the number of neurons keeps decreasing going from input to output.\n",
    "\n",
    "For a neural network with a single hidden layer, the number of neurons is:\n",
    "\n",
    "Nh = √n ∗ m\n",
    "Where:\n",
    "- n is the number of input features\n",
    "- m is the number of outputs\n",
    "\n",
    "While a neural network with two hidden layers the number of neurons can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nh = pow(INPUTS*OUTPUTS, 1/2)\n",
    "\n",
    "mlp_masters_1l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_masters_1l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh))\n",
    "print()\n",
    "\n",
    "mlp_masters_1l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = (n/m)^(1/3)\n",
    "r = pow(INPUTS/OUTPUTS, 1/3)   # = 4.279\n",
    "\n",
    "Nh1 = OUTPUTS*pow(r,2)    # = 183.179\n",
    "Nh2 = OUTPUTS*r           # = 42.799\n",
    "\n",
    "mlp_masters_2l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh1, activation='sigmoid'),\n",
    "    Dense(Nh2, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_masters_2l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:')\n",
    "print('    - 1st Layer:', math.trunc(Nh1))\n",
    "print('    - 2nd Layer:', math.trunc(Nh2))\n",
    "print()\n",
    "\n",
    "mlp_masters_2l.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamura and Tateishi (1997)\n",
    "Tamura and Tateish developed a method based on Akaike Information Criteria. The number of neurons in a three-layer neural network is N-1 and a four-layer neural network is (N/2)+3 where N is the number of inputs minus the number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = INPUTS-OUTPUTS              # 774\n",
    "Nh = N-1                        # 773\n",
    "\n",
    "mlp_tamura_3l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sis'),\n",
    "    Dense(Nh, activation='relu'),\n",
    "    Dense(Nh, activation='relu'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_tamura_3l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh), 'on each of the 3 layers')\n",
    "print()\n",
    "\n",
    "mlp_tamura_3l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = INPUTS-OUTPUTS              # 774\n",
    "Nh = (N/2)+3                    # 390\n",
    "\n",
    "mlp_tamura_4l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_tamura_4l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh), 'on each of the 4 layers')\n",
    "print()\n",
    "\n",
    "mlp_tamura_4l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # el shape de los inputs es alto_imagen * ancho_imagen * cantidad_colores\n",
    "    \n",
    "    Convolution2D(input_shape=(28, 28, 1), filters=8, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    # kernels de 4x4x1, y salida de 26x26x8\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Convolution2D(filters=8, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    # kernels de 4x4x8, y salida de 58x58x8\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(4, 4)),\n",
    "    # salida de 14x14x8\n",
    "    \n",
    "    Flatten(),\n",
    "    # salida de 1568\n",
    "    \n",
    "    Dense(10, activation='tanh'),\n",
    "    # salida de 10\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(10, activation='tanh'),\n",
    "    # salida de 10\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "    # salida de 10\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train, \n",
    "    epochs=5,\n",
    "    batch_size=500,\n",
    "    validation_data=(X_test, Y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e32d50fac4f93648707e6c06d16f383ac3ef03705b1d743a12bf83afaecbf40"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
