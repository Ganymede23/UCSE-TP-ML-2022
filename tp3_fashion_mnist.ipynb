{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holy search for Determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = 7\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "tf.keras.utils.set_random_seed(7)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "'''\n",
    "\n",
    "# Determinism\n",
    "tf.keras.utils.set_random_seed(7)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "tf.random.set_seed(7)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests GPU local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPUs:\", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de python, para especificar rutas de archivos y directorios\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "# lib para trabajar con arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# lib que usamos para mostrar las imágenes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# libs que usamos para construir y entrenar redes neuronales, y que además tiene utilidades para leer sets de \n",
    "# imágenes\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten, Rescaling\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "# libs que usamos para tareas generales de machine learning. En este caso, métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# configuración para que las imágenes se vean dentro del notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "'''\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "(X_train, Y_train) = train \n",
    "(X_test, Y_test) = test\n",
    "'''\n",
    "train, test = fashion_mnist.load_data()\n",
    "(X_train, Y_train) = train \n",
    "(X_test, Y_test) = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "\n",
    "LABELS = np.unique(Y_train).tolist()\n",
    "LABELS_TEXT = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "INPUTS = 28*28\n",
    "OUTPUTS = len(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "El dataset se divide en un set de train con 60.000 ejemplos, y un set de test con otros 10.000. Cada ejemplo consta de una imagen en escala de grises de 28x28 pixeles (784 en total), asociado a una etiqueta de 10 clases.\n",
    "Cada pixel es representado por un solo valor, indicando el nivel de brillo u obscuridad en él. Estos valores van entre 0 y 255.\n",
    "\n",
    "### Etiquetas\n",
    "- 0 - T-shirt/top\n",
    "- 1 - Trouser\n",
    "- 2 - Pullover\n",
    "- 3 - Dress\n",
    "- 4 - Coat\n",
    "- 5 - Sandal\n",
    "- 6 - Shirt\n",
    "- 7 - Sneaker\n",
    "- 8 - Bag\n",
    "- 9 - Ankle boot\n",
    "\n",
    "Las dimensiones de las imágenes pueden apreciarse al hacer un `.shape` de los datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:', X_train.shape)\n",
    "print('Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataset): \n",
    "    # specify the number of rows and columns you want to see\n",
    "    num_row = 3\n",
    "    num_col = 3\n",
    "\n",
    "    # get a segment of the dataset\n",
    "    num = num_row*num_col\n",
    "    if dataset == train:\n",
    "        images, labels = X_train[:num], Y_train[:num]\n",
    "    else: # Test dataset\n",
    "        images, labels = X_test[:num], Y_test[:num]\n",
    "\n",
    "    # plot images\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample_images(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ytrain = pd.DataFrame(data=Y_train)\n",
    "df_ytest = pd.DataFrame(data=Y_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\n",
    "fig.suptitle('Distribución de la variable target')\n",
    "ax1.bar([0,1,2,3,4,5,6,7,8,9],df_ytrain.value_counts())\n",
    "ax2.bar([0,1,2,3,4,5,6,7,8,9],df_ytest.value_counts())\n",
    "ax1.set_xticks([0,1,2,3,4,5,6,7,8,9])\n",
    "ax2.set_xticks([0,1,2,3,4,5,6,7,8,9])\n",
    "ax1.title.set_text('Train')\n",
    "ax2.title.set_text('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable a predecir tiene una **distribución uniforme** en todo ambos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de valores a un rango 0-1\n",
    "\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions\n",
    "    https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n",
    "\n",
    "Loss functions\n",
    "    https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "Neurons and layers\n",
    "    https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "    https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "\n",
    "https://www.hindawi.com/journals/mpe/2013/425740/\n",
    "https://peerj.com/articles/cs-724/\n",
    "\n",
    "    The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "    The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "    The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "\n",
    "> In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU …\n",
    "\n",
    "— Page 174, Deep Learning, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "VERBOSE = 0\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Fit model\n",
    "def fit_model(model, epochs=EPOCHS, batch_size=BATCH_SIZE, plot_model=True, verbose=VERBOSE, compare=False):\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if verbose == 0:\n",
    "        print('Trained the model for', epochs, 'epochs')\n",
    "\n",
    "        final_train_accuracy = model.history.history['accuracy'][-1]\n",
    "        final_validation_accuracy = model.history.history['val_accuracy'][-1]\n",
    "        final_train_loss = model.history.history['loss'][-1]\n",
    "        final_validation_loss = model.history.history['val_loss'][-1]\n",
    "\n",
    "        if compare:\n",
    "            initial_train_accuracy = model.history.history['accuracy'][0]\n",
    "            initial_validation_accuracy = model.history.history['val_accuracy'][0]\n",
    "            initial_train_loss = model.history.history['loss'][0]\n",
    "            initial_validation_loss = model.history.history['val_loss'][0]\n",
    "\n",
    "            print('- Train: \\t Initial Accuracy:', format(round(initial_train_accuracy, 4), '.4f'), \n",
    "                '\\t Final Accuracy:', format(round(final_train_accuracy, 4), '.4f'), \n",
    "                '\\t Initial Loss:', format(round(initial_train_loss, 4), '.4f'), \n",
    "                '\\t Final Loss:', format(round(final_train_loss, 4), '.4f'))\n",
    "            print('- Validation: \\t Initial Accuracy:', format(round(initial_validation_accuracy, 4), '.4f'), \n",
    "                '\\t Final Accuracy:', format(round(final_validation_accuracy, 4), '.4f'), \n",
    "                '\\t Initial Loss:', format(round(initial_validation_loss, 4), '.4f'), \n",
    "                '\\t Final Loss:', format(round(final_validation_loss, 4), '.4f'))    \n",
    "        else:\n",
    "            print('- Train: \\t Accuracy:', format(round(final_train_accuracy, 4), '.4f'), '\\t Loss:', format(round(final_train_loss, 4), '.4f'))\n",
    "            print('- Validation: \\t Accuracy:', format(round(final_validation_accuracy, 4), '.4f'), '\\t Loss:', format(round(final_validation_loss, 4), '.4f'))\n",
    "\n",
    "    return history\n",
    "\n",
    "# Plot train and validation accuracy of up to three models\n",
    "def plot_model(model_1, model_2=None, model_3=None, loss=False, title='', subtitle_1='', subtitle_2='', subtitle_3='', epochs=EPOCHS):\n",
    "    if loss:\n",
    "        ylabel = 'Loss'\n",
    "        train_metric = 'loss'\n",
    "        validation_metric = 'val_loss'\n",
    "    else:\n",
    "        ylabel = 'Accuracy'\n",
    "        train_metric = 'accuracy'\n",
    "        validation_metric = 'val_accuracy'\n",
    "\n",
    "    if model_2 is None and model_3 is None:\n",
    "        plt.figure(figsize=(7, 5)) \n",
    "        plt.title(title)\n",
    "        plt.plot(model_1.history[train_metric], label='train')\n",
    "        plt.plot(model_1.history[validation_metric], label='validation')\n",
    "        if epochs <= 20:\n",
    "            plt.xticks(np.arange(epochs), np.arange(1, epochs+1))\n",
    "        else:\n",
    "            original_ticks_list = np.arange(epochs)\n",
    "            # Just 3 ticks (min, half, max)\n",
    "            new_ticks_list = [min(original_ticks_list), original_ticks_list[math.floor(len(original_ticks_list)/2)], max(original_ticks_list)]\n",
    "            new_ticks_label_list = [min(original_ticks_list+1), original_ticks_list[math.floor(len(original_ticks_list)/2)], max(original_ticks_list)+1]\n",
    "            plt.xticks(new_ticks_list, new_ticks_label_list)\n",
    "       \n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        if model_3 is None:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15.3, 5))\n",
    "            fig.suptitle(title)\n",
    "            ax1.title.set_text(subtitle_1)\n",
    "            ax2.title.set_text(subtitle_2)\n",
    "            ax1.plot(model_1.history[train_metric], label='train')\n",
    "            ax2.plot(model_2.history[train_metric], label='train')\n",
    "            ax1.plot(model_1.history[validation_metric], label='validation')\n",
    "            ax2.plot(model_2.history[validation_metric], label='validation')\n",
    "\n",
    "            axes_list = [ax1, ax2]\n",
    "\n",
    "        else:\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(24, 5))\n",
    "            fig.suptitle(title)\n",
    "            ax1.title.set_text(subtitle_1)\n",
    "            ax2.title.set_text(subtitle_2)\n",
    "            ax3.title.set_text(subtitle_3)\n",
    "            ax1.plot(model_1.history[train_metric], label='train')\n",
    "            ax2.plot(model_2.history[train_metric], label='train')\n",
    "            ax3.plot(model_3.history[train_metric], label='train')\n",
    "            ax1.plot(model_1.history[validation_metric], label='validation')\n",
    "            ax2.plot(model_2.history[validation_metric], label='validation')\n",
    "            ax3.plot(model_3.history[validation_metric], label='validation')\n",
    "\n",
    "            ax2.tick_params(\n",
    "            axis='y',\n",
    "            which='both',   \n",
    "            left=False\n",
    "            )  \n",
    "\n",
    "            axes_list = [ax1, ax2, ax3]\n",
    "\n",
    "        for ax in axes_list:\n",
    "            if epochs <= 20:\n",
    "                ax.set_xticks(np.arange(epochs), np.arange(1, epochs+1))\n",
    "            else:\n",
    "                original_ticks_list = np.arange(epochs)\n",
    "                # Just 3 ticks (min, half, max)\n",
    "                new_ticks_list = [min(original_ticks_list), original_ticks_list[math.floor(len(original_ticks_list)/2)], max(original_ticks_list)]\n",
    "                new_ticks_label_list = [min(original_ticks_list+1), original_ticks_list[math.floor(len(original_ticks_list)/2)], max(original_ticks_list)+1]\n",
    "                ax.set_xticks(new_ticks_list, new_ticks_label_list)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.legend(loc='best')\n",
    "            ax.grid()\n",
    "\n",
    "            ax.tick_params(\n",
    "                axis='y',\n",
    "                which='both',   \n",
    "                right=False,\n",
    "                labelright=False,\n",
    "                left=True,\n",
    "                labelleft=True,\n",
    "            )\n",
    "\n",
    "# Plot train and validation accuracy of up to three models\n",
    "def plot_cm(model_1, model_2=None, model_3=None, dataset=train, title='', subtitle_1='', subtitle_2='', subtitle_3=''):\n",
    "    X_train, Y_train = dataset\n",
    "    labels = Y_train #Rename just for the sake of understanding\n",
    "\n",
    "    if model_2 is None and model_3 is None:\n",
    "        predictions = np.argmax(model_1.predict(X_train), axis=-1)\n",
    "        \n",
    "        print(' - Accuracy:', accuracy_score(labels, predictions))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        ax = sns.heatmap(confusion_matrix(labels, predictions), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted class') \n",
    "        plt.ylabel('True class') \n",
    "        \n",
    "    else:\n",
    "        if model_3 is None:\n",
    "            predictions_model_1 = np.argmax(model_1.predict(X_train), axis=-1)\n",
    "            predictions_model_2 = np.argmax(model_2.predict(X_train), axis=-1)\n",
    "\n",
    "            print(' -', subtitle_1,'Accuracy:', accuracy_score(labels, predictions_model_1))\n",
    "            print(' -', subtitle_2,'Accuracy:', accuracy_score(labels, predictions_model_2))\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15.3,5))\n",
    "            fig.suptitle(title)\n",
    "            ax1.title.set_text(subtitle_1)\n",
    "            ax2.title.set_text(subtitle_2)\n",
    "            axes_list = [ax1, ax2]\n",
    "            g1 = sns.heatmap(confusion_matrix(labels, predictions_model_1), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT, ax=ax1)\n",
    "            g2 = sns.heatmap(confusion_matrix(labels, predictions_model_2), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT, ax=ax2)\n",
    "\n",
    "        else:\n",
    "            predictions_model_1 = np.argmax(model_1.predict(X_train), axis=-1)\n",
    "            predictions_model_2 = np.argmax(model_2.predict(X_train), axis=-1)\n",
    "            predictions_model_3 = np.argmax(model_3.predict(X_train), axis=-1)\n",
    "\n",
    "            print(' -', subtitle_1,'Accuracy:', accuracy_score(labels, predictions_model_1))\n",
    "            print(' -', subtitle_2,'Accuracy:', accuracy_score(labels, predictions_model_2))\n",
    "            print(' -', subtitle_3,'Accuracy:', accuracy_score(labels, predictions_model_3))\n",
    "\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,5))\n",
    "            fig.suptitle(title)\n",
    "            ax1.title.set_text(subtitle_1)\n",
    "            ax2.title.set_text(subtitle_2)\n",
    "            ax3.title.set_text(subtitle_3)\n",
    "            axes_list = [ax1, ax2, ax3]\n",
    "            g1 = sns.heatmap(confusion_matrix(labels, predictions_model_1), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT, ax=ax1)\n",
    "            g2 = sns.heatmap(confusion_matrix(labels, predictions_model_2), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT, ax=ax2)\n",
    "            g3 = sns.heatmap(confusion_matrix(labels, predictions_model_3), cmap='Blues', annot=True, fmt='.0f', cbar=True, xticklabels=LABELS_TEXT, yticklabels=LABELS_TEXT, ax=ax3)\n",
    "\n",
    "        for ax in axes_list:\n",
    "            ax.set_xlabel('Predicted class')\n",
    "            ax.set_ylabel('True class')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {
    "imagen-2.png": {
     "image/png": "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"
    },
    "imagen-3.png": {
     "image/png": "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"
    },
    "imagen.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests académicos\n",
    "\n",
    "## Masters (1993)\n",
    "\n",
    "Masters argumenta en su libro *Practical Neural Network Recipes in C++* que no hay razones teóricas para utilizar más de dos capas ocultas en una red neuronal, como así tampoco razones prácticas. A su vez, propone utilizar la regla de la pirámide geométrica para determinar el número de neuronas ocultas, la cual establece que el número de neuronas para cada capa oculta formará una pirámide, donde el número de neuronas sigue disminuyendo de entrada a salida.\n",
    "\n",
    "Para una red neuronal con una sola capa oculta, la cantidad de neuronas se calcula de la siguiente manera:\n",
    "\n",
    "![imagen.png](attachment:imagen.png)\n",
    "\n",
    "Donde:\n",
    "- n es la cantidad de inputs\n",
    "- m es la cantidad de outputs\n",
    "\n",
    "Por otro lado, la cantidad de neuronas para una red con dos capas ocultas se calcula así:\n",
    "\n",
    "![imagen-2.png](attachment:imagen-2.png)\n",
    "\n",
    "Si se quisiera utilizar este mismo principio para una red con tres capas ocultas, a cantidad de neuronas en cada una de ellas se calcula de esta manera:\n",
    "\n",
    "![imagen-3.png](attachment:imagen-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nh = pow(INPUTS*OUTPUTS, 1/2)\n",
    "\n",
    "mlp_masters_1l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_masters_1l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh))\n",
    "print()\n",
    "\n",
    "mlp_masters_1l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_masters_1l_fit = fit_model(mlp_masters_1l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = (n/m)^(1/3)\n",
    "r = pow(INPUTS/OUTPUTS, 1/3)    # = 4.279\n",
    "                                \n",
    "Nh1 = OUTPUTS*pow(r,2)          # = 183.179\n",
    "Nh2 = OUTPUTS*r                 # = 42.799\n",
    "                            \n",
    "mlp_masters_2l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh1, activation='sigmoid'),\n",
    "    Dense(Nh2, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_masters_2l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:')\n",
    "print('    - 1st Layer:', math.trunc(Nh1))\n",
    "print('    - 2nd Layer:', math.trunc(Nh2))\n",
    "print()\n",
    "\n",
    "mlp_masters_2l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_masters_2l_fit = fit_model(mlp_masters_2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = (n/m)^(1/3)\n",
    "r = pow(INPUTS/OUTPUTS, 1/3)    # = 4.279\n",
    "                                \n",
    "Nh1 = OUTPUTS*pow(r,3)          # = 783.478\n",
    "Nh2 = OUTPUTS*pow(r,2)          # = 183.179\n",
    "Nh3 = OUTPUTS*r                 # = 42.799\n",
    "                            \n",
    "mlp_masters_3l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh1, activation='sigmoid'),\n",
    "    Dense(Nh2, activation='sigmoid'),\n",
    "    Dense(Nh3, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_masters_3l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:')\n",
    "print('    - 1st Layer:', math.trunc(Nh1))\n",
    "print('    - 2nd Layer:', math.trunc(Nh2))\n",
    "print('    - 3rd Layer:', math.trunc(Nh3))\n",
    "print()\n",
    "\n",
    "mlp_masters_3l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_masters_3l_fit = fit_model(mlp_masters_3l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between all three models based on Masters idea\n",
    "plot_model(model_1=mlp_masters_1l_fit, model_2=mlp_masters_2l_fit, model_3=mlp_masters_3l_fit, \n",
    "    title=\"Masters's proposed MLP models\", subtitle_1='1 Hidden Layer MLP', subtitle_2='2 Hidden Layers MLP', subtitle_3='3 Hidden Layers MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(mlp_masters_1l, mlp_masters_2l, mlp_masters_3l, \n",
    "    title=\"Masters's proposed MLP models\", subtitle_1='1 Hidden Layer MLP', subtitle_2='2 Hidden Layers MLP', subtitle_3='3 Hidden Layers MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamura and Tateishi (1997)\n",
    "Tamura and Tateishi desarrollaron un método basado en el Criterio de Información de Akaike. La cantidad de neuronas en una red neuronal tres capas es N-1, y en una red neuronal de cuatro capas, es (N/2)+3, donde N es resta entre la cantidad de inputs y outputs.\n",
    "\n",
    "Tamura and Tateishi developed a method to fix the number of hidden neuron. The number of hidden neurons in three layer neural network is N − 1 and four-layer neural network is N/2 + 3 where N is the input-target relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = INPUTS-OUTPUTS              # 774\n",
    "Nh = N-1                        # 773\n",
    "\n",
    "mlp_tamura_3l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_tamura_3l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh), 'on each of the 3 layers')\n",
    "print()\n",
    "\n",
    "mlp_tamura_3l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tamura_3l_fit = fit_model(mlp_tamura_3l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = INPUTS-OUTPUTS              # 774\n",
    "Nh = (N/2)+3                    # 390\n",
    "\n",
    "mlp_tamura_4l = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(Nh, activation='sigmoid'),\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_tamura_4l.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "print('Hidden neurons:', math.trunc(Nh), 'on each of the 4 layers')\n",
    "print()\n",
    "\n",
    "mlp_tamura_4l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_tamura_4l_fit = fit_model(mlp_tamura_4l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison between two models based on Tamura and Tateishi's model\n",
    "plot_model(model_1=mlp_tamura_3l_fit, model_2=mlp_tamura_4l_fit, \n",
    "    title=\"Tamura and Tateishi's proposed MLP models\", subtitle_1='3 Hidden Layers MLP', subtitle_2='4 Hidden Layers MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(model_1=mlp_tamura_3l, model_2=mlp_tamura_4l, \n",
    "    title=\"Tamura and Tateishi's proposed MLP models\", subtitle_1='3 Hidden Layers MLP', subtitle_2='4 Hidden Layers MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Experimentation\n",
    "\n",
    "Para experimentar con diversos aspectos de redes neuronales MLP vamos a buscar variar no solo su arquitectura -en términos de cantidad de capas ocultas y neuronas- sino también con la cantidad de Epochs, tamaño del Batch Size y learning rate utilizados.\n",
    "\n",
    "Primero comenzamos planteando modelos muy pequeños, de entre 1 y 3 capas ocultas respectivamente, y con 2 neuronas cada una de ellas. El objetivo de esto es observar el comportamiento de una red neuronal cuando ya no tiene capacidad de seguir aprendiendo. Las observaciones resultantes pueden ayudarnos a comprender de mejor manera el comportamiento de modelos más complejos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1l_2_relu = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_1l_2_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_1l_2_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2l_2_relu = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_2l_2_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_2l_2_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_2_relu = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_2_relu.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_2_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mlp_1l_1_relu')\n",
    "mlp_1l_2_relu_b256 = fit_model(mlp_1l_2_relu, batch_size=256, epochs=100, compare=True)\n",
    "print('mlp_2l_1_relu')\n",
    "mlp_2l_2_relu_b256 = fit_model(mlp_2l_2_relu, batch_size=256, epochs=100, compare=True)\n",
    "print('mlp_3l_1_relu')\n",
    "mlp_3l_2_relu_b256 = fit_model(mlp_3l_2_relu, batch_size=256, epochs=100, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_1l_2_relu_b256, model_2=mlp_2l_2_relu_b256, model_3=mlp_3l_2_relu_b256, epochs=100,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 2 perceptrons (ReLU)', subtitle_2='2 Layers, 2 perceptrons each (ReLU)', subtitle_3='3 Layers, 2 perceptrons each (ReLU)')\n",
    "plot_model(model_1=mlp_1l_2_relu_b256, model_2=mlp_2l_2_relu_b256, model_3=mlp_3l_2_relu_b256, epochs=100, loss=True,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 1 perceptrons (ReLU)', subtitle_2='2 Layers, 2 perceptrons each (ReLU)', subtitle_3='3 Layers, 2 perceptrons each (ReLU)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con tangente hiperbólica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1l_2_tanh = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_1l_2_tanh.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_1l_2_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2l_2_tanh = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_2l_2_tanh.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_2l_2_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_2_tanh = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(2, activation='tanh'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_2_tanh.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_2_tanh.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mlp_1l_2_tanh')\n",
    "mlp_1l_2_tanh_b256 = fit_model(mlp_1l_2_tanh, batch_size=256, epochs=100, compare=True)\n",
    "print('mlp_2l_2_tanh')\n",
    "mlp_2l_2_tanh_b256 = fit_model(mlp_2l_2_tanh, batch_size=256, epochs=100, compare=True)\n",
    "print('mlp_3l_2_tanh')\n",
    "mlp_3l_2_tanh_b256 = fit_model(mlp_3l_2_tanh, batch_size=256, epochs=100, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_1l_2_tanh_b256, model_2=mlp_2l_2_tanh_b256, model_3=mlp_3l_2_tanh_b256, epochs=100,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 1 perceptron', subtitle_2='2 Layers, 1 perceptron each', subtitle_3='3 Layers, 1 perceptron each')\n",
    "plot_model(model_1=mlp_1l_2_tanh_b256, model_2=mlp_2l_2_tanh_b256, model_3=mlp_3l_2_tanh_b256, epochs=100, loss=True,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 1 perceptron', subtitle_2='2 Layers, 1 perceptron each', subtitle_3='3 Layers, 1 perceptron each')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de entrenar los modelos repetidas veces pudimos observar que en buena parte de los casos el Accuracy tiene una asíntota entre el 0.50 y 0.52, mientras que en otros casos esta métrica queda estancada en 0.48.\n",
    "Estos modelos iniciales utilizan la tangente hiperbólica como función de activación.\n",
    "\n",
    "Si bien inicialmente optamos por ReLU, algunas veces los casos quedaban estancados durante todo su entrenamiento con el Accuracy en 0.1 y el Loss en 2.3. Una de las hipótesis sobre la causa de esto es 'Dying ReLU problem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1l_2 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_1l_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_1l_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_1l_2_b256 = fit_model(mlp_1l_2, batch_size=256, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2l_2 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_2l_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_2l_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2l_2_b256 = fit_model(mlp_2l_2, batch_size=256, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_2 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_2_b256 = fit_model(mlp_3l_2, batch_size=256, compare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_1l_2_b256, model_2=mlp_2l_2_b256, model_3=mlp_3l_2_b256,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 2 perceptrons', subtitle_2='2 Layers, 2 perceptrons each', subtitle_3='3 Layers, 2 perceptron each')\n",
    "plot_model(model_1=mlp_1l_2_b256, model_2=mlp_2l_2_b256, model_3=mlp_3l_2_b256, loss=True,\n",
    "    title='Accuracy over train epochs', subtitle_1='1 Layer, 2 perceptrons', subtitle_2='2 Layers, 2 perceptrons each', subtitle_3='3 Layers, 2 perceptrons each')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_e10 = fit_model(mlp_3l_100, epochs=10)\n",
    "mlp_3l_100_e20 = fit_model(mlp_3l_100, epochs=20)\n",
    "mlp_3l_100_e30 = fit_model(mlp_3l_100, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_e10, model_2=mlp_3l_100_e20, model_3=mlp_3l_100_e30, epochs=30, \n",
    "    title='Accuracy over train epochs', subtitle_1='Epochs=10', subtitle_2='Epochs=20', subtitle_3='Epochs=30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_e10, model_2=mlp_3l_100_e20, model_3=mlp_3l_100_e30, epochs=30, loss=True,\n",
    "    title='Accuracy over train epochs', subtitle_1='Epochs=10', subtitle_2='Epochs=20', subtitle_3='Epochs=30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_b64 = fit_model(mlp_3l_100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_b256 = fit_model(mlp_3l_100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_b1024 = fit_model(mlp_3l_100, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_b64, model_2=mlp_3l_100_b256, model_3=mlp_3l_100_b1024,\n",
    "    title='Accuracy over train epochs', subtitle_1='Batch Size=64', subtitle_2='Batch Size=256', subtitle_3='Batch Size=1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_b64, model_2=mlp_3l_100_b256, model_3=mlp_3l_100_b1024, loss=True, \n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=64', subtitle_2='Batch Size=256', subtitle_3='Batch Size=1024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de un `batch_size` más alto tiene los siguientes efectos:\n",
    "\n",
    "| batch_size | Tiempo (s)  |\n",
    "| ---------- | ----------- |\n",
    "| 64         | 58          |\n",
    "| 256        | 26          |\n",
    "| 1024       | 20          |\n",
    "\n",
    "- El entrenamiento es, naturalmente, más rápido. \n",
    "- El accuracy de train aumenta a medida que aumenta el `batch_size` y hace overfitting.\n",
    "- El loss de Validation continúa aumentando respecto al de Train a medida que pasan los `epochs`. Esto es otro indicativo de overfitting.\n",
    "\n",
    "Estas observaciones nos indican que el modelo actual, con tres capas ocultas de 100 neuronas cada una, es tiene suficiente capacidad como para memorizar los datos de Train. En este caso tenemos varias alternativas:\n",
    "1. Añadir capas de dropout.\n",
    "2. Reducir la cantidad de capas y/o neuronas.\n",
    "3. Añadir más datos.\n",
    "\n",
    "Procedemos a testear la primer situación, utilizando capas de Dropout entre las capas ocultas de la red.\n",
    "\n",
    "### Test con Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_d02 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100_d02.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100_d02.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_d02_b64 = fit_model(mlp_3l_100_d02, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_d02_b256 = fit_model(mlp_3l_100_d02, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_d02_b1024 = fit_model(mlp_3l_100_d02, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_d02_b64, model_2=mlp_3l_100_d02_b256, model_3=mlp_3l_100_d02_b1024, \n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=64, Dropout=0.2', subtitle_2='Batch Size=256, Dropout=0.2', subtitle_3='Batch Size=1024, Dropout=0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_d02_b64, model_2=mlp_3l_100_d02_b256, model_3=mlp_3l_100_d02_b1024, loss=True, \n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=64, Dropout=0.2', subtitle_2='Batch Size=256, Dropout=0.2', subtitle_3='Batch Size=1024, Dropout=0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La adición de tres capas de Dropout controló el crecimiento del loss de Validation y añadió ciertas irregularidades en las métricas, al menos comparándolo con los gráficos más 'suaves' obtenidos en el ejemplo anterior.\n",
    "Esto es entendible debido a la naturaleza de cómo opera Dropout, al asignar valores 0 en los inputs de manera random con determinada frecuencia. Además, también subió levemente el piso del loss en Train.\n",
    "\n",
    "| batch_size | Tiempo (s)  |\n",
    "| ---------- | ----------- |\n",
    "| 64         | 72          |\n",
    "| 256        | 40          |\n",
    "| 1024       | 33          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test con menor cantidad de neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_10.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_b64 = fit_model(mlp_3l_10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_b256 = fit_model(mlp_3l_10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_b1024 = fit_model(mlp_3l_10, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_b64, model_2=mlp_3l_10_b256, model_3=mlp_3l_10_b1024,\n",
    "    title='Accuracy over train epochs', subtitle_1='Batch Size=64', subtitle_2='Batch Size=256', subtitle_3='Batch Size=1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_b64, model_2=mlp_3l_10_b256, model_3=mlp_3l_10_b1024, loss=True, \n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=64', subtitle_2='Batch Size=256', subtitle_3='Batch Size=1024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al reducir la cantidad de neuronas en un 90% (de 100 a 10) pero manteniendo las tres capas densas iniciales alcanzamos valores muy similares independientemente del `batch_size`. De todas maneras, en el 2do y 3er ejemplo el loss de Validation sigue incrementándose mientras el de Train se mantiene estable, lo cual indica un overfitting.\n",
    "\n",
    "A partir de este momento tomamos como base al segundo modelo (con `batch_size = 256`) para continuar con la experimentación.\n",
    "\n",
    "El hecho de que el loss no cambie a lo largo de los `epochs` es llamativo. Esto ocurrió cuando redujimos la cantidad de neuronas, por lo que podríamos suponer que la red ya no tiene capacidad de aprender. \n",
    "Procedemos a realizar dos cambios:\n",
    "1. Disminuir el "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_lr00001 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_10_lr00001.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001, #Default 0.001\n",
    "    ),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_10_lr00001.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_lr00001_b256 = fit_model(mlp_3l_10_lr00001, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_4l_10 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_4l_10.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_4l_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_4l_10_b256 = fit_model(mlp_4l_10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_lr00001_b256, model_2=mlp_3l_10_b256, model_3=mlp_4l_10_b256,\n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=256, Learning Rate=0.0001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_lr00001_b256, model_2=mlp_3l_10_b256, model_3=mlp_4l_10_b256, loss=True, \n",
    "    title='Loss over train epochs', subtitle_1='Batch Size=256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr0001 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100_lr0001.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001, #Default\n",
    "    ),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100_lr0001.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr0001_fit = fit_model(mlp_3l_100_lr0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr001 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100_lr001.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.01,\n",
    "    ),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100_lr001.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr001_fit = fit_model(mlp_3l_100_lr001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr01 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_100_lr01.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.1,\n",
    "    ),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_100_lr01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_100_lr01_fit = fit_model(mlp_3l_100_lr01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_lr0001_fit, model_2=mlp_3l_100_lr001_fit, model_3=mlp_3l_100_lr01_fit, \n",
    "    title='Accuracy over train epochs with different learning rates', subtitle_1='Learning Rate=0.001', subtitle_2='Learning Rate=0.01', subtitle_3='Learning Rate=0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_100_lr0001_fit, model_2=mlp_3l_100_lr001_fit, model_3=mlp_3l_100_lr01_fit, loss=True,\n",
    "    title='Accuracy over train epochs with different learning rates', subtitle_1='Learning Rate=0.001', subtitle_2='Learning Rate=0.01', subtitle_3='Learning Rate=0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede apreciarse que al utilizar un learning rate 10 veces más grande al default (0.01, en lugar de 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_10.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_20 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_20.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_20.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_40 = Sequential([\n",
    "    Rescaling(1/255, input_shape=(28,28,1)),\n",
    "    Flatten(),\n",
    "    Dense(40, activation='relu'),\n",
    "    Dense(40, activation='relu'),\n",
    "    Dense(40, activation='relu'),\n",
    "    Dense(OUTPUTS, activation='softmax'),\n",
    "])\n",
    "\n",
    "mlp_3l_40.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "mlp_3l_40.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3l_10_fit = fit_model(mlp_3l_10)\n",
    "mlp_3l_20_fit = fit_model(mlp_3l_20)\n",
    "mlp_3l_40_fit = fit_model(mlp_3l_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_fit, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_1=mlp_3l_10_fit, model_2=mlp_3l_20_fit, model_3=mlp_3l_40_fit,\n",
    "    title=\"MLP TEST NO. 1\", subtitle_1='MOD1', subtitle_2='MOD2', subtitle_3='MOD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search MLP\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "    'hidden_layer_sizes': [x for x in itertools.product((10,50,100),repeat=3)],\n",
    "    'activation': [\"logistic\", \"relu\", \"tanh\"]\n",
    "}  \n",
    "base_estimator = MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape de X_train a 2D para GridSearchCV\n",
    "feature_vector_length = 784\n",
    "num_classes = 10\n",
    "\n",
    "X_trainR = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
    "X_testR = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
    "\n",
    "import itertools\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "# defining parameter range \n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,10),(10,10,10)],\n",
    "    'batch_size': [128, 256, 512],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'alpha': 10.0 ** -np.arange(1, 5),\n",
    "    \n",
    "    'max_iter': [30],\n",
    "    'solver': ['adam'],\n",
    "}  \n",
    "\n",
    "'''\n",
    "grid2 = GridSearchCV(MLPClassifier(), param_grid=param_grid, n_jobs=-1, verbose=3, cv=3)\n",
    "\n",
    "# fitting the model for grid search \n",
    "grid2.fit(X_trainR, Y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid2.best_params_) \n",
    "grid_predictions = grid2.predict(X_testR) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(Y_test, grid_predictions)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # el shape de los inputs es alto_imagen * ancho_imagen * cantidad_colores\n",
    "    \n",
    "    Convolution2D(input_shape=(28, 28, 1), filters=8, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    # kernels de 4x4x1, y salida de 26x26x8\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Convolution2D(filters=8, kernel_size=(4, 4), strides=1, activation='relu'),\n",
    "    # kernels de 4x4x8, y salida de 58x58x8\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(4, 4)),\n",
    "    # salida de 14x14x8\n",
    "    \n",
    "    Flatten(),\n",
    "    # salida de 1568\n",
    "    \n",
    "    Dense(10, activation='tanh'),\n",
    "    # salida de 10\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(10, activation='tanh'),\n",
    "    # salida de 10\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(len(LABELS), activation='softmax'),\n",
    "    # salida de 10\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy',],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train, \n",
    "    epochs=5,\n",
    "    batch_size=500,\n",
    "    validation_data=(X_test, Y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prunning\n",
    "\n",
    "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e32d50fac4f93648707e6c06d16f383ac3ef03705b1d743a12bf83afaecbf40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
