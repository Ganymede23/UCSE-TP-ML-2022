{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y Evaluación de Modelos\n",
    "\n",
    "## Trabajo Práctico Nro. 2 - Grupo 3\n",
    "\n",
    "#### Integrantes:\n",
    "* Ignacio Busso\n",
    "* Lucas Copes\n",
    "* Jesica Heit\n",
    "\n",
    "#### Dataset: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction\n",
    "* Detalle: contiene datos de la satisfacción de los pasajeros de diferentes vuelos tomando en cuenta multiples aspectos (calidad del servicio, comodidad, limpieza, etc.)\n",
    "* Target: columna 'satisfaction', para determinar la satisfacción de un pasajero respecto a un vuelo.\n",
    "* Dimensiones: 25 columnas x 129.880 filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_columns = 0\n",
    "\n",
    "#Cambios en el estilo de los graficos\n",
    "plt.style.use('fast')\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura y concatenación de los .csv\n",
    "train = pd.read_csv('data/train.csv', index_col=[0])\n",
    "test = pd.read_csv('data/test.csv', index_col=[0])\n",
    "all_data = pd.concat([train, test], sort=False)\n",
    "\n",
    "# Asignamos nuevos nombres a algunas de las columnas\n",
    "new_column_names = {\n",
    "    'Gender': 'gender',\n",
    "    'Customer Type': 'customer_type',\n",
    "    'Age': 'age',\n",
    "    'Type of Travel': 'business_travel',\n",
    "    'Class': 'ticket_class',\n",
    "    'Flight Distance': 'flight_distance',\n",
    "    'Inflight wifi service': 'wifi_service',\n",
    "    'Departure/Arrival time convenient': 'departure_arrival_time_convenient',\n",
    "    'Ease of Online booking': 'online_booking',\n",
    "    'Gate location': 'gate_location',\n",
    "    'Food and drink': 'food_and_drink',\n",
    "    'Online boarding': 'online_boarding',\n",
    "    'Seat comfort': 'seat_comfort',\n",
    "    'Inflight entertainment': 'inflight_entertainment',\n",
    "    'On-board service': 'onboard_service',\n",
    "    'Leg room service': 'leg_room',\n",
    "    'Baggage handling': 'baggage_handling',\n",
    "    'Checkin service': 'checkin',\n",
    "    'Inflight service': 'inflight_service',\n",
    "    'Cleanliness': 'cleanliness',\n",
    "    'Departure Delay in Minutes': 'departure_delay',\n",
    "    'Arrival Delay in Minutes': 'arrival_delay',\n",
    "}\n",
    "\n",
    "all_data.rename(columns=new_column_names, inplace=True)\n",
    "all_data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrica\n",
    "\n",
    "Accuracy es una métrica para evaluar los modelos de clasificación. Mide el porcentaje de casos donde el modelo acertó. Es una de las métricas más usadas.\n",
    "\n",
    "Nuestro grupo decide utilizar esta métrica ya que la información no se encuentra desbalanceada debido a que disponemos de un 56% de personas que se encuentran “neutral or dissatisfied” y un 43% de personas que se encuentran “satisfied”. Debido a que nuestras clases se encuentran muy bien balanceadas, Accuracy es una buena métrica a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado\n",
    "1. Conversión de variables cualitativas a booleanas cuantitativas.\n",
    "2.  \n",
    "    - Limpieza de registros con nulos implícitos en los servicios. Aplica solo a features con menos de 500 casos.\n",
    "    - Limpieza de registros NaN en `arrival_delays`. (~400 registros)\n",
    "3. Limpieza de valores extremos en `arrival_delays` y `departure_delays`. (~600 registros)\n",
    "4. Limpieza de valores extremos en `flight_distance`. Aplica solo a vuelos de más de 4000 unidades de longitud (~75 registros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión a variables booleanas\n",
    "all_data['gender'] = all_data['gender'].replace(['Male','Female'],[0,1])\n",
    "all_data['customer_type'] = all_data['customer_type'].replace(['disloyal Customer','Loyal Customer'],[0,1])\n",
    "all_data['business_travel'] = all_data['business_travel'].replace(['Personal Travel','Business travel'],[0,1])\n",
    "all_data['satisfaction'] = all_data['satisfaction'].replace(['neutral or dissatisfied','satisfied'],[0,1])\n",
    "\n",
    "# Limpieza de filas con pocas (< 500) features de servicios nulas (== 0) y arrivals NaNs\n",
    "all_data = all_data[\n",
    "    ~(all_data == 0).gate_location &\n",
    "    ~(all_data == 0).food_and_drink &\n",
    "    ~(all_data == 0).seat_comfort & \n",
    "    ~(all_data == 0).inflight_entertainment &\n",
    "    ~(all_data == 0).onboard_service &\n",
    "    ~(all_data == 0).checkin &\n",
    "    ~(all_data == 0).inflight_service &\n",
    "    ~(all_data == 0).cleanliness &\n",
    "    ~all_data.arrival_delay.isnull()\n",
    "    ]\n",
    "\n",
    "# Limpieza de valores extremos en Arrivals y Departures\n",
    "all_data = all_data.drop(all_data[all_data.arrival_delay > 240].index)\n",
    "all_data = all_data.drop(all_data[all_data.departure_delay > 240].index)\n",
    "\n",
    "# Limpieza de valores extremos en flight_distance (> 4000 - 75 registros)\n",
    "all_data = all_data.drop(all_data[all_data.flight_distance > 4000].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Se crean una serie de intervalos numéricos para cuatro variables. Los valores son reemplazados por el mayor número de cada intervalo. Ej: Si un viaje es de 150km, el valor es reemplazado por 500km, ya que cae dentro del intervalo de 0 a 500.\n",
    "Variables involucradas: `age`, `flight_distance`, `departure_delay` y `arrival_delay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de intervalos para features numéricas\n",
    "    # Puede ser visto como Binning / Redondeo de números? Feature Engineering?\n",
    "    # Ver de dejarlo para el final. Ejecutarlo por separado en otro df y comparar resultados.\n",
    "\n",
    "all_data_fe = all_data.copy(deep=True)\n",
    "\n",
    "# ¿Corresponde que los labels tengan como nombre el mayor numero del intervalo? ¿Debería usar un valor calculado en su lugar? (Media/Mediana/Moda)\n",
    "all_data_fe[\"age_interval\"] = pd.cut(x=all_data_fe['age'], bins=[0,12,18,30,60,80,100], labels=[12,18,30,60,80,100])\n",
    "all_data_fe[\"flight_distance_interval\"] = pd.cut(x=all_data_fe['flight_distance'], bins=[0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000], labels=[500,1000,1500,2000,2500,3000,3500,4000,4500,5000])\n",
    "all_data_fe[\"departure_delay_interval\"] = pd.cut(x=all_data_fe['departure_delay'], bins=[-1,0,15,30,45,60,120,180,240], labels=[0,15,30,45,60,120,180,240])\n",
    "all_data_fe[\"arrival_delay_interval\"] = pd.cut(x=all_data_fe['arrival_delay'], bins=[-1,0,15,30,45,60,120,180,240], labels=[0,15,30,45,60,120,180,240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado de datasets\n",
    "- 60% Train         (77k)\n",
    "- 20% Validation    (25k)\n",
    "- 20% Test          (25k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def armado_datasets(df):\n",
    "    train, not_train = train_test_split(df, test_size=0.4, random_state=7)\n",
    "    validation, test = train_test_split(not_train, test_size=0.5, random_state=7)\n",
    "\n",
    "    train.shape, validation.shape, test.shape\n",
    "    return train, validation, test\n",
    "\n",
    "train, validation, test = armado_datasets(all_data)\n",
    "train_fe, validation_fe, test_fe = armado_datasets(all_data_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameMapper\n",
    "- Escalar todas las variables numéricas no booleanas con MinMaxScaler (valores entre 0-1).\n",
    "- Imputar las features con más de 600 nulos con IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, QuantileTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Imputar y escalar\n",
    "mapper = DataFrameMapper([\n",
    "    (['gender'], None),\n",
    "    (['customer_type'], None),\n",
    "    (['age'], [MinMaxScaler()]),\n",
    "    (['business_travel'], None),\n",
    "    (['ticket_class'], [OneHotEncoder()]),\n",
    "    (['flight_distance'], [MinMaxScaler()]),\n",
    "], df_out=True)\n",
    "mapper.fit(train)\n",
    "\n",
    "mapper_fe = DataFrameMapper([\n",
    "    (['gender'], None),\n",
    "    (['customer_type'], None),\n",
    "    (['age_interval'], [MinMaxScaler()]),\n",
    "    (['business_travel'], None),\n",
    "    (['ticket_class'], [OneHotEncoder()]),\n",
    "    (['flight_distance_interval'], [MinMaxScaler()]),\n",
    "], df_out=True)\n",
    "mapper_fe.fit(train_fe)\n",
    "\n",
    "mapper_fe_qt = DataFrameMapper([\n",
    "    (['gender'], None),\n",
    "    (['customer_type'], None),\n",
    "    (['age'], [QuantileTransformer(), MinMaxScaler()]),\n",
    "    (['business_travel'], None),\n",
    "    (['ticket_class'], [OneHotEncoder()]),\n",
    "    (['flight_distance'], [QuantileTransformer(), MinMaxScaler()]),\n",
    "], df_out=True)\n",
    "mapper_fe_qt.fit(train)\n",
    "\n",
    "#mfit = mapper.fit(all_data)\n",
    "#all_data_transformed = mapper.transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de un sample:\n",
    "sample = train.sample(3, random_state=7)\n",
    "sample_fe = train_fe.sample(3, random_state=7)\n",
    "sample_fe_qt = train.sample(3, random_state=7)\n",
    "# Sample original:\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_fe.transform(sample_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_fe_qt.transform(sample_fe_qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "Serán utilizados los siguientes algoritmos:\n",
    "1. Regresión Logística\n",
    "2. Arboles de Decisión\n",
    "3. Gradient Boosting\n",
    "\n",
    "En este punto se entrenan y evalúan los modelos elegidos utilizando tanto parámetros estándar como algunos de los hiperparámetros principales de cada uno de ellos. Al finalizar, en base a las métricas y el comportamiento general se arriba a conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_model(model, feature_engineering=False, test=False ,set_names=('train', 'validation'), title='', show_cm=True):\n",
    "    if title:\n",
    "        display(title)\n",
    "        \n",
    "    final_metrics = defaultdict(list)\n",
    "\n",
    "    if feature_engineering:\n",
    "        set_names=('train_fe', 'validation_fe')\n",
    "    else:\n",
    "        set_names=('train', 'validation')\n",
    "        \n",
    "    if test:\n",
    "        set_names=('train', 'validation', 'test')\n",
    "    \n",
    "    if show_cm:\n",
    "        fig, axis = plt.subplots(1, len(set_names), sharey=True, figsize=(15, 3))\n",
    "    \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        if not feature_engineering:\n",
    "            assert set_name in ['train', 'validation', 'test']\n",
    "        else:\n",
    "            assert set_name in ['train_fe', 'validation_fe', 'test_fe']\n",
    "        \n",
    "        set_data = globals()[set_name]\n",
    "\n",
    "        y = set_data.satisfaction\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Accuracy'].append(metrics.accuracy_score(y, y_pred))\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        final_metrics['Recall'].append(metrics.recall_score(y, y_pred))\n",
    "        final_metrics['F1'].append(metrics.f1_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            ax = axis[i]\n",
    "            sns.heatmap(metrics.confusion_matrix(y, y_pred), ax=ax, cmap='Blues', annot=True, fmt='.0f', cbar=False)\n",
    "\n",
    "            ax.set_title(set_name)\n",
    "            ax.xaxis.set_ticklabels(['Not Satisfied', 'Satisfied'])\n",
    "            ax.yaxis.set_ticklabels(['Not Satisfied', 'Satisfied'])\n",
    "            ax.set_xlabel('Predicted class')\n",
    "            ax.set_ylabel('True class')\n",
    "\n",
    "    display(pd.DataFrame(final_metrics, index=set_names))\n",
    "    if show_cm:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística - Parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard\n",
    "regression_model = LogisticRegression(random_state=7)\n",
    "lr_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model, title='Regresión Lineal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística - Iteraciones máximas\n",
    "\n",
    "Entrenamiento limitando la cantidad de iteraciones máximas que el algoritmo puede hacer antes de converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iter = 2\n",
    "regression_model = LogisticRegression(max_iter=2, random_state=7)\n",
    "lr_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model, title='Regresión Lineal - max_iter = 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica Accuracy cae ~3 puntos porcentuales en este caso, ya que al limitar la cantidad de iteraciones forzamos al algoritmo a converger antes de tiempo, cuando todavía hay lugar para mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para graficar los arboles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz  # pip install graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def graph_tree(tree, col_names):\n",
    "    graph_data = export_graphviz(\n",
    "        tree, \n",
    "        out_file=None, \n",
    "        feature_names=col_names,  \n",
    "        class_names=['Neutral/Not Satisfied', 'Satisfied'],  \n",
    "        filled=True, \n",
    "        rounded=True,  \n",
    "        special_characters=True,\n",
    "    )\n",
    "    graph = graphviz.Source(graph_data)  \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de Decisión - Parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "tree_model = DecisionTreeClassifier(random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del entrenamiento sin definir parámetros es un overfitting bastante claro en Train, con todas las métricas por encima del 97%. Esto se debe principalmente al hecho de que no se le ha definido una profundidad máxima, permitiendo así que el arbol siga memorizando los ejemplos y creando una rama para cada uno de ellos.\n",
    "\n",
    "Puede verse que el arbol llega a tener una profundidad maxima de 56 niveles y un total de 46033 (59% de los registros del dataset de Train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de Decisión - Profundidad máxima\n",
    "**Entrenamiento definiendo un `max_depth` de 4 (Default = None).** De esta manera buscamos eliminar el overfitting visto en el caso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 4\n",
    "tree_model = DecisionTreeClassifier(max_depth=4,random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree - max_depth = 4')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con un límite en la cantidad máxima de niveles permitida se alcanzan valores más realistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento definiendo un `max_depth` de 1 (valor mínimo).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 1\n",
    "tree_model = DecisionTreeClassifier(max_depth=1,random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree - max_depth = 1')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar el valor mínimo de profundidad acarrea a que el Accuracy caiga 4 puntos porcentuales. Naturalmente, esto ocurre ya que acotamos demasiado la cantidad de cortes que puede hacer el árbol de decisión.\n",
    "\n",
    "En el siguiente gráfico podemos ver que la clasificación se hace solamente en base al valor de la feature `online_boarding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#graph_tree(tree_model, mapper.transformed_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Entrenamiento con parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "gradient_model = GradientBoostingClassifier(random_state=7)\n",
    "gb_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model, title='Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Learning rate\n",
    "Entrenamiento con un `learning_rate` mucho mayor (20, en lugar del 0.1 por default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 20\n",
    "gradient_model = GradientBoostingClassifier(learning_rate=20, random_state=7)\n",
    "gb_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model, title='Gradient Boosting - learning_rate = 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto resulta en una métrica Accuracy 50 puntos porcentuales más baja que la anterior, ya que el modelo falla y clasifica erroneamente a muchos de los casos. Por otro lado, el tiempo de ejecución se ve afectado solo mínimamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Cantidad de árboles\n",
    "**Entrenamiento definiendo un `n_estimators` de 4 (Default = 100).** Este parámetro indica la cantidad de arboles que Gradient Boosting va a modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = 4\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=4, random_state=7)\n",
    "gb_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model, title='Gradient Boosting - n_estimators = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso las métricas dan apenas uno o dos puntos porcentuales menos que la ejecución inicial sin parámetros. Esto parece indicar que el rendimiento del modelo llega a una meseta a partir de los 4 árboles creados.\n",
    "\n",
    "**Entrenamiento definiendo un `n_estimators` de 3 (Default = 100).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = 2\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=2, random_state=7)\n",
    "gb_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model, title='Gradient Boosting - n_estimators = 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segunda prueba y con solo dos árboles creados, las métricas se mantienen muy similares al ejemplo anterior. Esto refuerza la teoría de que se forma una meseta a partir de, ahora, el segundo árbol utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se evalúa cada uno de los tres algoritmos elegidos modificando continuamente alguno de sus hiperparámetros principales a lo largo de varias iteraciones, mientras se compara el valor de la métrica Accuracy en los datasets Train y Validation. En base a esta información se selecciona la configuración óptima de hiperparámetros para los modelos a entrenar en el resto del trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_test(model, set_names=('train', 'validation')):\n",
    "    final_metrics = defaultdict(list)\n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train', 'validation', 'test']\n",
    "        set_data = globals()[set_name]\n",
    "\n",
    "        y = set_data.satisfaction\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Accuracy'].append(metrics.accuracy_score(y, y_pred))\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística\n",
    "- Parámetro modificado: `max_iter`\n",
    "- Valores: 1 a 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "validation_acc = []\n",
    "for i in range(1,21):\n",
    "    regression_model = LogisticRegression(max_iter=i, random_state=7)\n",
    "    lr_model = Pipeline([\n",
    "        ('mapper', mapper),\n",
    "        ('classifier', regression_model),\n",
    "    ])\n",
    "    lr_model.fit(train, train.satisfaction)\n",
    "\n",
    "    final_metrics = overfitting_test(lr_model)\n",
    "    train_acc.append(final_metrics['Accuracy'][0])\n",
    "    validation_acc.append(final_metrics['Accuracy'][1])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_acc, label='Train')\n",
    "plt.plot(validation_acc, label='Validation')\n",
    "plt.axvline(x=6, color='red', linestyle=':') # 6 es x=7\n",
    "plt.title('Accuracy respecto a la cantidad máxima de iteraciones')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Cantidad máxima de iteraciones')\n",
    "plt.xticks(np.arange(len(train_acc)), np.arange(1, len(train_acc)+1))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo final se realiza el corte del entrenamiento al llegar a las 7 iteraciones, punto máximo antes de que la performance caiga y se vuelva un tanto inestable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de Decisión\n",
    "- Parámetro modificado: `max_depth`\n",
    "- Valores: 1 a 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "validation_acc = []\n",
    "for i in range(1,21):\n",
    "    tree_model = DecisionTreeClassifier(max_depth=i, random_state=7)\n",
    "    dt_model = Pipeline([\n",
    "        ('mapper', mapper),\n",
    "        ('classifier', tree_model),\n",
    "    ])\n",
    "    dt_model.fit(train, train.satisfaction)\n",
    "\n",
    "    final_metrics = overfitting_test(dt_model)\n",
    "    train_acc.append(final_metrics['Accuracy'][0])\n",
    "    validation_acc.append(final_metrics['Accuracy'][1])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_acc, label='Train')\n",
    "plt.plot(validation_acc, label='Validation')\n",
    "plt.axvline(x=5, color='red', linestyle=':') # 5 es x=6\n",
    "plt.title('Accuracy respecto a la profundidad máxima')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Profundidad máxima')\n",
    "plt.xticks(np.arange(len(train_acc)), np.arange(1, len(train_acc)+1))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo final se realiza el corte a la profundidad máxima de 6, antes de que la divergencia entre los Accuracy de Train y Validation continúe acrecentándose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "- Parámetro modificado: `n_estimators`\n",
    "- Valores: 1 a 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "validation_acc = []\n",
    "for i in range(1,21):\n",
    "    gradient_model = GradientBoostingClassifier(n_estimators=i, random_state=7)\n",
    "    gb_model = Pipeline([\n",
    "        ('mapper', mapper),\n",
    "        ('classifier', gradient_model),\n",
    "    ])\n",
    "    gb_model.fit(train, train.satisfaction)\n",
    "\n",
    "    final_metrics = overfitting_test(gb_model)\n",
    "    train_acc.append(final_metrics['Accuracy'][0])\n",
    "    validation_acc.append(final_metrics['Accuracy'][1])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_acc, label='Train')\n",
    "plt.plot(validation_acc, label='Validation')\n",
    "plt.axvline(x=1, color='red', linestyle=':') # 1 es x=2\n",
    "plt.title('Accuracy respecto a la cantidad máxima de árboles')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Cantidad máxima de árboles')\n",
    "plt.xticks(np.arange(len(train_acc)), np.arange(1, len(train_acc)+1))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo final se realiza el corte al llegar a 2 árboles creados, y el max_depth se mantiene por default en 3. No tiene sentido utilizar una cantidad mayor ya que las metricas tienden a una meseta total en iteraciones posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos definitivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística sin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iter = 10\n",
    "regression_model = LogisticRegression(max_iter=10, random_state=7)\n",
    "lr_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model, title='Regresión Lineal sin Feature Engineering - max_iter = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística con Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_iter = 10\n",
    "regression_model = LogisticRegression(max_iter=10, random_state=7)\n",
    "lr_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model_fe.fit(train_fe, train_fe.satisfaction)\n",
    "evaluate_model(lr_model_fe, feature_engineering=True, title='Regresión Lineal con Feature Engineering (intervalos creados a mano) - max_iter = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística con Feature Engineering - Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_iter = 10\n",
    "regression_model = LogisticRegression(max_iter=10, random_state=7)\n",
    "lr_model_fe_qt = Pipeline([\n",
    "    ('mapper', mapper_fe_qt),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model_fe_qt.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model_fe_qt, title='Regresión Lineal con Feature Engineering (Quantile Transformation) - max_iter = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr_model, title='Regresión Logística sin Feature Engineering', show_cm=False)\n",
    "evaluate_model(lr_model_fe,feature_engineering= True, title='Regresión Logística con Feature Engineering - Intervalos a mano', show_cm=False)\n",
    "evaluate_model(lr_model_fe_qt, title='Regresión Logística con Feature Engineering - Quantile Transformation', show_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de decisión sin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_depth = 10\n",
    "tree_model = DecisionTreeClassifier(max_depth=10,random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Árbol de Decisión sin Feature Engineering - max_depth = 10')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de decisión con Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth = 10\n",
    "tree_model = DecisionTreeClassifier(max_depth=10,random_state=7)\n",
    "dt_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model_fe.fit(train_fe, train_fe.satisfaction)\n",
    "evaluate_model(dt_model_fe, feature_engineering=True, title='Árbol de Decisión con Feature Engineering (intervalos creados a mano) - max_depth = 10')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol de decisión con Feature Engineering - Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_depth = 10\n",
    "tree_model = DecisionTreeClassifier(max_depth=10,random_state=7)\n",
    "dt_model_fe_qt = Pipeline([\n",
    "    ('mapper', mapper_fe_qt),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model_fe_qt.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model_fe_qt, title='Árbol de Decisión con Feature Engineering (Quantile Transformation) - max_depth = 10')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_model, title='Árbol de Decisión sin Feature Engineering', show_cm=False)\n",
    "evaluate_model(dt_model_fe,feature_engineering= True, title='Árbol de Decisión con Feature Engineering - Intervalos a mano', show_cm=False)\n",
    "evaluate_model(dt_model_fe_qt, title='Árbol de Decisión con Feature Engineering - Quantile Transformation', show_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting sin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = 4\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=4, random_state=7)\n",
    "gb_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model, title='Gradient Boosting sin Feature Engineering - n_estimators = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting con Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_estimators = 4\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=4, random_state=7)\n",
    "gb_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model_fe.fit(train_fe, train_fe.satisfaction)\n",
    "evaluate_model(gb_model_fe, feature_engineering=True, title='Gradient Boosting con Feature Engineering (intervalos creados a mano) - n_estimators = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting con Feature Engineering - Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_estimators = 4\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=4, random_state=7)\n",
    "gb_model_fe_qt = Pipeline([\n",
    "    ('mapper', mapper_fe_qt),\n",
    "    ('classifier', gradient_model),\n",
    "])\n",
    "\n",
    "gb_model_fe_qt.fit(train, train.satisfaction)\n",
    "evaluate_model(gb_model_fe_qt, title='Gradient Boosting con Feature Engineering (Quantile Transformation) - n_estimators = 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gb_model, title='Gradient Boosting sin Feature Engineering', show_cm=False)\n",
    "evaluate_model(gb_model_fe,feature_engineering= True, title='Gradient Boosting con Feature Engineering - Intervalos a mano', show_cm=False)\n",
    "evaluate_model(gb_model_fe_qt, title='Gradient Boosting con Feature Engineering - Quantile Transformation', show_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al examinar los tres modelos utilizando tres diferentes variantes del mismo Dataset, las métricas muestran que el uso de Quantile Transformation como Feature Engineering no aporta ninguna ventaja por sobre el Dataset estándar, mientras que el Feature Engineering manual por intervalos emperoa las métricas ligeramente. Por lo tanto, se decide realizar la evaluación de los modelos utilizando el Dataset estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación final entre modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr_model, title='Regresión Logística', show_cm=False)\n",
    "evaluate_model(dt_model, title='Árbol de Decisión', show_cm=False)\n",
    "evaluate_model(gb_model, title='Gradient Boosting', show_cm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = dt_model\n",
    "evaluate_model(chosen_model, test= True, title='Modelo elegido', set_names=('train', 'validation', 'test'), show_cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de analizar los distintos métodos utilizados, podemos ver que según qué variables de entrada utilizamos para entrenar nuestro modelo, los resultados pueden variar bastante. Si elegimos como entrada las variables del puntaje de cada servicio ofrecido, las métricas en general funcionan con un Accuracy mayor al 89%. Esto es porque resulta fácil deducir una salida de satisfacción final teniendo en cuenta el puntaje de cada apartado por separado como entrada. \n",
    "Ahora, si usamos solo las variables del pasajero y datos básicos del vuelo, podemos ver una situación más cercana al mundo real, ya que no estamos tratando de evaluar el puntaje sino que las variables que se tienen en cuenta no están en un principio tan relacionadas con el target. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1f9169ae4315cfc4614565a4c2b2d600515b6454333995dfdf26296b7266cb1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
