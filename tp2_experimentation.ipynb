{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y Evaluación de Modelos\n",
    "\n",
    "## Trabajo Práctico Nro. 2 - Grupo 3\n",
    "\n",
    "#### Integrantes:\n",
    "* Ignacio Busso\n",
    "* Lucas Copes\n",
    "* Jesica Heit\n",
    "\n",
    "#### Dataset: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction\n",
    "* Detalle: contiene datos de la satisfacción de los pasajeros de diferentes vuelos tomando en cuenta multiples aspectos (calidad del servicio, comodidad, limpieza, etc.)\n",
    "* Target: columna 'satisfaction', para determinar la satisfacción de un pasajero respecto a un vuelo.\n",
    "* Dimensiones: 25 columnas x 129.880 filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_columns = 0\n",
    "\n",
    "#Cambios en el estilo de los graficos\n",
    "plt.style.use('fast')\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura y concatenación de los .csv\n",
    "train = pd.read_csv('data/train.csv', index_col=[0])\n",
    "test = pd.read_csv('data/test.csv', index_col=[0])\n",
    "all_data = pd.concat([train, test], sort=False)\n",
    "\n",
    "# Asignamos nuevos nombres a algunas de las columnas\n",
    "new_column_names = {\n",
    "    'Gender': 'gender',\n",
    "    'Customer Type': 'customer_type',\n",
    "    'Age': 'age',\n",
    "    'Type of Travel': 'business_travel',\n",
    "    'Class': 'ticket_class',\n",
    "    'Flight Distance': 'flight_distance',\n",
    "    'Inflight wifi service': 'wifi_service',\n",
    "    'Departure/Arrival time convenient': 'departure_arrival_time_convenient',\n",
    "    'Ease of Online booking': 'online_booking',\n",
    "    'Gate location': 'gate_location',\n",
    "    'Food and drink': 'food_and_drink',\n",
    "    'Online boarding': 'online_boarding',\n",
    "    'Seat comfort': 'seat_comfort',\n",
    "    'Inflight entertainment': 'inflight_entertainment',\n",
    "    'On-board service': 'onboard_service',\n",
    "    'Leg room service': 'leg_room',\n",
    "    'Baggage handling': 'baggage_handling',\n",
    "    'Checkin service': 'checkin',\n",
    "    'Inflight service': 'inflight_service',\n",
    "    'Cleanliness': 'cleanliness',\n",
    "    'Departure Delay in Minutes': 'departure_delay',\n",
    "    'Arrival Delay in Minutes': 'arrival_delay',\n",
    "}\n",
    "\n",
    "all_data.rename(columns=new_column_names, inplace=True)\n",
    "all_data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrica\n",
    "\n",
    "F-Score es una medida de precisión de un modelo de datos. Se utiliza para evaluar los sistemas de clasificación binaria, estos se clasifican en positivos o negativos.\n",
    "Combina precission (se refiere al número de verdaderos positivos dividido por el número total de predicciones positivas (el número de verdaderos positivos más el número de falsos positivos)) y recall (calcular los elementos relevantes (el número de verdaderos positivos dividido el número de elementos relevantes)\n",
    "F-Score es la media armónica entre precission y recall.\n",
    "\n",
    "Cuando F-Score tiende a estar más cercano al 1 quiere decir que precission y recall son muy buenos, en cambio, cuando tiende a 0 quiere decir que ambos métodos son muy malos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado\n",
    "1. Conversión de variables cualitativas a booleanas cuantitativas.\n",
    "2. \n",
    "    - Limpieza de registros con nulos implícitos en los servicios. Aplica solo a features con menos de 500 casos.\n",
    "    - Limpieza de registros NaN en `arrival_delays`. (~400 registros)\n",
    "3. Limpieza de valores extremos en `arrival_delays` y `departure_delays`. (~600 registros)\n",
    "4. Limpieza de valores extremos en `flight_distance`. Aplica solo a vuelos de más de 4000 unidades de longitud (~75 registros)\n",
    "5. Feature Engineering creando intervalos para cuatro variables numéricas. (`age`, `flight_distance`, `departure_delay` y `arrival_delay`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión a variables booleanas\n",
    "all_data['gender'] = all_data['gender'].replace(['Male','Female'],[0,1])\n",
    "all_data['customer_type'] = all_data['customer_type'].replace(['disloyal Customer','Loyal Customer'],[0,1])\n",
    "all_data['business_travel'] = all_data['business_travel'].replace(['Personal Travel','Business travel'],[0,1])\n",
    "all_data['satisfaction'] = all_data['satisfaction'].replace(['neutral or dissatisfied','satisfied'],[0,1])\n",
    "\n",
    "# Limpieza de filas con pocas (< 500) features de servicios nulas (== 0) y arrivals NaNs\n",
    "all_data = all_data[\n",
    "    ~(all_data == 0).gate_location &\n",
    "    ~(all_data == 0).food_and_drink &\n",
    "    ~(all_data == 0).seat_comfort & \n",
    "    ~(all_data == 0).inflight_entertainment &\n",
    "    ~(all_data == 0).onboard_service &\n",
    "    ~(all_data == 0).checkin &\n",
    "    ~(all_data == 0).inflight_service &\n",
    "    ~(all_data == 0).cleanliness &\n",
    "    ~all_data.arrival_delay.isnull()\n",
    "    ]\n",
    "\n",
    "# Limpieza de valores extremos en Arrivals y Departures\n",
    "all_data = all_data.drop(all_data[all_data.arrival_delay > 240].index)\n",
    "all_data = all_data.drop(all_data[all_data.departure_delay > 240].index)\n",
    "\n",
    "# Limpieza de valores extremos en flight_distance (> 4000 - 75 registros)\n",
    "all_data = all_data.drop(all_data[all_data.flight_distance > 4000].index)\n",
    "\n",
    "# Creación de intervalos para features numéricas\n",
    "    # Puede ser visto como Binning / Redondeo de números? Feature Engineering?\n",
    "    # Ver de dejarlo para el final. Ejecutarlo por separado en otro df y comparar resultados.\n",
    "\n",
    "# ¿Corresponde que los labels tengan como nombre el mayor numero del intervalo? ¿Debería usar un valor calculado en su lugar? (Media/Mediana/Moda)\n",
    "all_data[\"age_interval\"] = pd.cut(x=all_data['age'], bins=[0,12,18,30,60,80,100], labels=[12,18,30,60,80,100])\n",
    "all_data[\"flight_distance_interval\"] = pd.cut(x=all_data['flight_distance'], bins=[0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000], labels=[500,1000,1500,2000,2500,3000,3500,4000,4500,5000])\n",
    "all_data[\"departure_delay_interval\"] = pd.cut(x=all_data['departure_delay'], bins=[-1,0,15,30,45,60,120,180,240], labels=[0,15,30,45,60,120,180,240])\n",
    "all_data[\"arrival_delay_interval\"] = pd.cut(x=all_data['arrival_delay'], bins=[-1,0,15,30,45,60,120,180,240], labels=[0,15,30,45,60,120,180,240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armado de datasets\n",
    "- 60% Train         (77k)\n",
    "- 20% Validation    (25k)\n",
    "- 20% Test          (25k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, not_train = train_test_split(all_data, test_size=0.4, random_state=7)\n",
    "validation, test = train_test_split(not_train, test_size=0.5, random_state=7)\n",
    "\n",
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrameMapper\n",
    "- Escalar todas las variables numéricas no booleanas con MinMaxScaler (valores entre 0-1).\n",
    "- Imputar las features con más de 600 nulos con IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Imputar y escalar\n",
    "mapper = DataFrameMapper([\n",
    "    (['gender'], None),\n",
    "    (['customer_type'], None),\n",
    "    (['age'], [MinMaxScaler()]),\n",
    "    (['business_travel'], None),\n",
    "    (['ticket_class'], [OneHotEncoder()]),\n",
    "    (['flight_distance'], [MinMaxScaler()]),\n",
    "    (['wifi_service'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['departure_arrival_time_convenient'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['online_booking'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['gate_location'], [MinMaxScaler()]),\n",
    "    (['food_and_drink'], [MinMaxScaler()]),\n",
    "    (['online_boarding'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['seat_comfort'], [MinMaxScaler()]),\n",
    "    (['inflight_entertainment'], [MinMaxScaler()]),\n",
    "    (['onboard_service'], [MinMaxScaler()]),\n",
    "    (['leg_room'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['baggage_handling'], [MinMaxScaler()]),\n",
    "    (['checkin'], [MinMaxScaler()]),\n",
    "    (['inflight_service'], [MinMaxScaler()]),\n",
    "    (['cleanliness'], [MinMaxScaler()]),\n",
    "    (['departure_delay'], [MinMaxScaler()]),\n",
    "    (['arrival_delay'], [MinMaxScaler()]),\n",
    "], df_out=True)\n",
    "mapper.fit(train)\n",
    "\n",
    "fe_mapper = DataFrameMapper([\n",
    "    (['gender'], None),\n",
    "    (['customer_type'], None),\n",
    "    (['age_interval'], [MinMaxScaler()]),\n",
    "    (['business_travel'], None),\n",
    "    (['ticket_class'], [OneHotEncoder()]),\n",
    "    (['flight_distance_interval'], [MinMaxScaler()]),\n",
    "    (['wifi_service'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['departure_arrival_time_convenient'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['online_booking'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['gate_location'], [MinMaxScaler()]),\n",
    "    (['food_and_drink'], [MinMaxScaler()]),\n",
    "    (['online_boarding'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['seat_comfort'], [MinMaxScaler()]),\n",
    "    (['inflight_entertainment'], [MinMaxScaler()]),\n",
    "    (['onboard_service'], [MinMaxScaler()]),\n",
    "    (['leg_room'], [IterativeImputer(missing_values=0), MinMaxScaler()]),\n",
    "    (['baggage_handling'], [MinMaxScaler()]),\n",
    "    (['checkin'], [MinMaxScaler()]),\n",
    "    (['inflight_service'], [MinMaxScaler()]),\n",
    "    (['cleanliness'], [MinMaxScaler()]),\n",
    "    (['departure_delay_interval'], [MinMaxScaler()]),\n",
    "    (['arrival_delay_interval'], [MinMaxScaler()]),\n",
    "], df_out=True)\n",
    "fe_mapper.fit(train)\n",
    "\n",
    "#mfit = mapper.fit(all_data)\n",
    "#all_data_transformed = mapper.transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de un sample:\n",
    "sample = train.sample(7, random_state=7)\n",
    "# Sample original:\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transformado.\n",
    "mapper.transform(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos\n",
    "Serán utilizados los siguientes algoritmos:\n",
    "1. Regresión Logística\n",
    "2. Arboles de Decisión\n",
    "3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Funcion Fisa\n",
    "def evaluate_model(model, set_names=('train', 'validation'), title='', show_cm=True):\n",
    "    if title:\n",
    "        display(title)\n",
    "        \n",
    "    final_metrics = defaultdict(list)\n",
    "    \n",
    "    if show_cm:\n",
    "        fig, axis = plt.subplots(1, len(set_names), sharey=True, figsize=(15, 3))\n",
    "    \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train', 'validation', 'test']\n",
    "        set_data = globals()[set_name]  # <- hack feo...\n",
    "\n",
    "        y = set_data.satisfaction\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Accuracy'].append(metrics.accuracy_score(y, y_pred))\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        final_metrics['Recall'].append(metrics.recall_score(y, y_pred))\n",
    "        final_metrics['F1'].append(metrics.f1_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            ax = axis[i]\n",
    "            sns.heatmap(metrics.confusion_matrix(y, y_pred), ax=ax, cmap='Blues', annot=True, fmt='.0f', cbar=False)\n",
    "\n",
    "            ax.set_title(set_name)\n",
    "            ax.xaxis.set_ticklabels(['not happy', 'happy'])\n",
    "            ax.yaxis.set_ticklabels(['not happy', 'happy'])\n",
    "            ax.set_xlabel('Predicted class')\n",
    "            ax.set_ylabel('True class')\n",
    "\n",
    "    display(pd.DataFrame(final_metrics, index=set_names))\n",
    "    if show_cm:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística - Parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = LogisticRegression(random_state=7)\n",
    "lr_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model, title='Regresión Lineal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística - Iteraciones máximas\n",
    "\n",
    "Entrenamiento limitando la cantidad de iteraciones máximas que el algoritmo puede hacer antes de converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = LogisticRegression(random_state=7, max_iter=2)\n",
    "lr_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', regression_model),\n",
    "])\n",
    "\n",
    "lr_model.fit(train, train.satisfaction)\n",
    "evaluate_model(lr_model, title='Regresión Lineal - max_iter = 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica Accuracy cae ~12 puntos porcentuales en este caso, ya que al limitar la cantidad de iteraciones forzamos al algoritmo a converger antes de tiempo, cuando todavía hay lugar para mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para graficar los arboles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz  # pip install graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def graph_tree(tree, col_names):\n",
    "    graph_data = export_graphviz(\n",
    "        tree, \n",
    "        out_file=None, \n",
    "        feature_names=col_names,  \n",
    "        class_names=['Neutral/Not Satisfied', 'Satisfied'],  \n",
    "        filled=True, \n",
    "        rounded=True,  \n",
    "        special_characters=True,\n",
    "    )\n",
    "    graph = graphviz.Source(graph_data)  \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de Decisión - Parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del entrenamiento sin definir parámetros es un overfitting bastante claro. Esto se debe principalmente al hecho de que no se le ha definido una profundidad máxima, permitiendo así que el arbol siga memorizando los ejemplos y creando una rama para cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de Decisión - Profundidad máxima\n",
    "Entrenamiento definiendo un `max_depth` de 4 (Default = None). De esta manera buscamos eliminar el overfitting visto en el caso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth=4,random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree - max_depth = 4')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con un límite en la cantidad máxima de niveles permitida, se alcanzan valores más realistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento definiendo un `max_depth` de 1 (valor mínimo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(max_depth=1,random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', tree_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Decision Tree - max_depth = 1')\n",
    "\n",
    "print('Cantidad de nodos:', tree_model.tree_.node_count)\n",
    "print('Profundidad máxima:', tree_model.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar el valor mínimo de profundidad acarrea a que el Accuracy caiga 8 puntos porcentuales. Naturalmente, esto ocurre ya que acotamos demasiado la cantidad de cortes que puede hacer el árbol de decisión.\n",
    "En el siguiente gráfico podemos ver que la clasificación se hace solamente en base al valor de la feature `online_boarding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_tree(tree_model, mapper.transformed_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Entrenamiento con parámetros estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gb_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Learning rate\n",
    "Entrenamiento con un learning rate mucho mayor (20, en lugar del 0.1 por default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(learning_rate=20, random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gb_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Gradient Boosting - learning_rate = 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto resulta en una métrica Accuracy mucho más baja que la anterior, ya que el modelo falla y clasifica erroneamente a muchos de los casos. Por otro lado, el tiempo de ejecución se ve afectado solo mínimamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting - Cantidad de árboles\n",
    "Entrenamiento con un `n_estimators` mucho menor (2, en lugar de los 100 por default). Este parámetro indica la cantidad de arboles que Gradient Boosting va a modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=2, random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gb_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Gradient Boosting - n_estimators = 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto resulta en una disminución de aproximadamente 12 puntos porcentuales en la métrica Accuracy al momento de predecir la variable target (comparado con la ejecución por default). Al limitar la cantidad de arboles de Gradient Boosting estamos afectando una de sus principales fortalezas: la de aprender de los errores del modelo anterior. \n",
    "\n",
    "Por eso mismo este ejemplo falla más al momento de predecir, y también tiene un Recall considerablemente más bajo al original, lo que muestra que con solo dos árboles existe todavía una cantidad considerable de falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=3, random_state=7)\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', gb_model),\n",
    "])\n",
    "\n",
    "dt_model.fit(train, train.satisfaction)\n",
    "evaluate_model(dt_model, title='Gradient Boosting - n_estimators = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con respecto al modelo anterior, al añadir un tercer árbol ya el Recall sube 20 puntos porcentuales (eliminando parte considerable de los falsos negativos), mientras que el Accuracy sube 6."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1f9169ae4315cfc4614565a4c2b2d600515b6454333995dfdf26296b7266cb1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
